
========================================================================
SELF-METRICS
========================================================================
SHA-256: eb31b8641c1eb08abbdaf1033c0b52cc7d4db77fa87fde3935e60930f2471182
Text metrics:
  - total_lines: 240
  - nonempty_lines: 206
  - blank_lines: 34
  - comment_lines: 27
  - total_chars: 18061
  - total_bytes_utf8: 18073
  - avg_line_length: 75.254
  - longest_line_length: 9180
  - trailing_whitespace_lines: 0
  - leading_tab_lines: 0
  - has_bom: False
  - word_count: 2283
  - unique_words: 365
  - unique_byte_values: 88
  - entropy_bits_per_byte: 4.8894
  - newline_counts: {'CRLF': 0, 'LF': 240, 'CR': 0}
  - char_classes: {'letters': 10705, 'digits': 130, 'whitespace': 3412, 'punct': 3814}
  - top_bytes (value, count): [(32, 3172), (110, 1206), (116, 1040), (101, 1026), (115, 854), (45, 802), (105, 790), (111, 650), (114, 631), (61, 588)]
AST metrics:
  - function_count: 7
  - class_count: 0
  - import_count: 7
  - rough_cyclomatic: 32
  - node_type_counts_top10: [('Load', 334), ('Name', 331), ('Constant', 131), ('Call', 106), ('Store', 76), ('Attribute', 54), ('Assign', 48), ('Expr', 27), ('comprehension', 18), ('Subscript', 17)]
  - top_identifiers: [('n', 24), ('ast', 24), ('src', 19), ('print', 19), ('len', 12), ('ln', 12), ('sum', 11), ('a', 10), ('tree', 9), ('t', 9)]
  - function_spans:
      * divider: lines 65..70
      * sha256_hex: lines 72..73
      * compute_text_metrics: lines 76..145
      * compute_ast_metrics: lines 147..187
      * print_metrics: lines 189..219
      * print_full_file_quine: lines 222..227
      * main: lines 230..237
  - longest_function_span: ('compute_text_metrics', 76, 145)

========================================================================
FULL-FILE QUINE (no file I/O)
========================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
===============================================================================
self_reflecting_quine.py
===============================================================================

OVERVIEW
--------
This single file does two main things:

  1) Self-metrics
     It reconstructs its own exact source code in memory and reports a suite
     of metrics: line and byte counts, hashes, newline style, trailing
     whitespace, longest line, byte histogram, a rough entropy estimate,
     and Python AST-based statistics (functions, classes, node counts,
     a rough cyclomatic complexity proxy, and top identifiers).

  2) Full-file quine (no file I/O)
     It prints its entire source code without reading from disk. The
     mechanism is a classic self-referential template: a single placeholder
     (written here as %r when discussed in comments) is replaced by the
     string-literal representation of the template itself.

WHY CARE
--------
Self-reference is the structural seed behind many fundamental limits in logic
and computation. A full-file quine demonstrates controlled self-reference in
code form; self-metrics show practical introspection on that very source.

HOW TO RUN
----------
    python self_reflecting_quine.py

OUTPUT STRUCTURE
----------------
1) SELF-METRICS      — counts, hashes, newline style, entropy, AST stats
2) FULL-FILE QUINE   — exact source printed with no disk reads
3) DONE

IMPORTANT NOTE ABOUT THE TEMPLATE
---------------------------------
Inside the FRAME string below, every literal percent sign must be written as
'%' so it appears as a single percent in the final printed source. There is
exactly one active placeholder: '%r'. Do not add lone percent signs inside
FRAME. Outside FRAME (normal Python code) you can use ordinary formatting.
===============================================================================
"""

# The quine template: this entire file’s text lives in FRAME with one placeholder
# %r that will be replaced (by printf-style formatting) with the repr() of FRAME.
# Any literal percent that should appear in the printed source must be written as
# '%' here so it survives formatting.
FRAME = '#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n"""\n===============================================================================\nself_reflecting_quine.py\n===============================================================================\n\nOVERVIEW\n--------\nThis single file does two main things:\n\n  1) Self-metrics\n     It reconstructs its own exact source code in memory and reports a suite\n     of metrics: line and byte counts, hashes, newline style, trailing\n     whitespace, longest line, byte histogram, a rough entropy estimate,\n     and Python AST-based statistics (functions, classes, node counts,\n     a rough cyclomatic complexity proxy, and top identifiers).\n\n  2) Full-file quine (no file I/O)\n     It prints its entire source code without reading from disk. The\n     mechanism is a classic self-referential template: a single placeholder\n     (written here as %%r when discussed in comments) is replaced by the\n     string-literal representation of the template itself.\n\nWHY CARE\n--------\nSelf-reference is the structural seed behind many fundamental limits in logic\nand computation. A full-file quine demonstrates controlled self-reference in\ncode form; self-metrics show practical introspection on that very source.\n\nHOW TO RUN\n----------\n    python self_reflecting_quine.py\n\nOUTPUT STRUCTURE\n----------------\n1) SELF-METRICS      — counts, hashes, newline style, entropy, AST stats\n2) FULL-FILE QUINE   — exact source printed with no disk reads\n3) DONE\n\nIMPORTANT NOTE ABOUT THE TEMPLATE\n---------------------------------\nInside the FRAME string below, every literal percent sign must be written as\n\'%%\' so it appears as a single percent in the final printed source. There is\nexactly one active placeholder: \'%%r\'. Do not add lone percent signs inside\nFRAME. Outside FRAME (normal Python code) you can use ordinary formatting.\n===============================================================================\n"""\n\n# The quine template: this entire file’s text lives in FRAME with one placeholder\n# %%r that will be replaced (by printf-style formatting) with the repr() of FRAME.\n# Any literal percent that should appear in the printed source must be written as\n# \'%%\' here so it survives formatting.\nFRAME = %r\n\nimport ast\nimport hashlib\nimport math\nimport re\nimport sys\nfrom collections import Counter\nfrom textwrap import indent\n\n# ------------------------------ Utilities ------------------------------------\ndef divider(title=None):\n    bar = "=" * 72\n    if title:\n        print(f"\\n{bar}\\n{title}\\n{bar}")\n    else:\n        print(f"\\n{bar}")\n\ndef sha256_hex(data: str) -> str:\n    return hashlib.sha256(data.encode("utf-8")).hexdigest()\n\n# ------------------------------ Metrics --------------------------------------\ndef compute_text_metrics(src: str) -> dict:\n    # Basic counts\n    lines = src.splitlines()\n    total_lines = len(lines)\n    total_chars = len(src)\n    total_bytes = len(src.encode("utf-8"))\n    nonempty = sum(1 for ln in lines if ln.strip())\n    blank = total_lines - nonempty\n    comment = sum(1 for ln in lines if re.match(r"^\\s*#", ln))\n\n    # Whitespace styles\n    trailing_ws = sum(1 for ln in lines if len(ln) and ln.rstrip() != ln)\n    leading_tabs = sum(1 for ln in lines if ln.startswith("\\t"))\n    has_bom = 1 if src.startswith("\\ufeff") else 0\n\n    # Newline style (heuristic counts)\n    crlf = src.count("\\r\\n")\n    lf = src.count("\\n") - crlf\n    cr = src.count("\\r") - crlf\n\n    # Line lengths\n    longest = max((len(ln) for ln in lines), default=0)\n    avg_line = (total_chars / total_lines) if total_lines else 0.0\n\n    # Token-ish word counts\n    words = re.findall(r"[A-Za-z0-9_]+", src)\n    word_count = len(words)\n    uniq_words = len(set(w.lower() for w in words))\n\n    # Character classes\n    classes = {\n        "letters": sum(c.isalpha() for c in src),\n        "digits": sum(c.isdigit() for c in src),\n        "whitespace": sum(c.isspace() for c in src),\n        "punct": total_chars - sum(c.isalnum() or c.isspace() for c in src),\n    }\n\n    # Byte stats\n    b = src.encode("utf-8")\n    byte_counts = Counter(b)\n    top_bytes = byte_counts.most_common(10)\n    unique_byte_values = len(byte_counts)\n\n    # Rough Shannon entropy (bits per byte)\n    total_b = len(b) or 1\n    entropy = 0.0\n    for _, cnt in byte_counts.items():\n        p = cnt / total_b\n        entropy -= p * math.log2(p)\n\n    return {\n        "total_lines": total_lines,\n        "nonempty_lines": nonempty,\n        "blank_lines": blank,\n        "comment_lines": comment,\n        "total_chars": total_chars,\n        "total_bytes_utf8": total_bytes,\n        "avg_line_length": round(avg_line, 3),\n        "longest_line_length": longest,\n        "trailing_whitespace_lines": trailing_ws,\n        "leading_tab_lines": leading_tabs,\n        "has_bom": bool(has_bom),\n        "newline_counts": {"CRLF": crlf, "LF": lf, "CR": cr},\n        "word_count": word_count,\n        "unique_words": uniq_words,\n        "char_classes": classes,\n        "unique_byte_values": unique_byte_values,\n        "entropy_bits_per_byte": round(entropy, 4),\n        "top_bytes": top_bytes,\n    }\n\ndef compute_ast_metrics(src: str) -> dict:\n    tree = ast.parse(src)\n    counts = Counter(type(n).__name__ for n in ast.walk(tree))\n\n    funcs = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]\n    classes = [n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]\n    imports = [n for n in ast.walk(tree) if isinstance(n, (ast.Import, ast.ImportFrom))]\n\n    # Rough cyclomatic proxy: decisions + boolean ops + comprehensions + 1\n    decisions = sum(isinstance(n, (ast.If, ast.For, ast.While, ast.Try, ast.With)) for n in ast.walk(tree))\n    bool_ops = sum(isinstance(n, ast.BoolOp) for n in ast.walk(tree))\n    comps = sum(isinstance(n, (ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp)) for n in ast.walk(tree))\n    rough_cyclomatic = 1 + decisions + bool_ops + comps\n\n    # Identifier frequencies\n    names = [n.id for n in ast.walk(tree) if isinstance(n, ast.Name)]\n    top_identifiers = Counter(names).most_common(10)\n\n    # Function line spans (if available)\n    def_spans = []\n    for n in funcs:\n        start = getattr(n, "lineno", None)\n        end = getattr(n, "end_lineno", None)\n        if start is not None:\n            def_spans.append((n.name, start, end))\n\n    # Longest function by line span\n    longest_fn = None\n    if def_spans:\n        longest_fn = max(def_spans, key=lambda t: (t[2] or t[1]) - t[1] + 1 if t[2] else 0)\n\n    return {\n        "node_type_counts_top10": counts.most_common(10),\n        "function_count": len(funcs),\n        "class_count": len(classes),\n        "import_count": len(imports),\n        "rough_cyclomatic": rough_cyclomatic,\n        "top_identifiers": top_identifiers,\n        "function_spans": def_spans,\n        "longest_function_span": longest_fn,\n    }\n\ndef print_metrics(src: str):\n    divider("SELF-METRICS")\n    print(f"SHA-256: {sha256_hex(src)}")\n\n    t = compute_text_metrics(src)\n    a = compute_ast_metrics(src)\n\n    print("Text metrics:")\n    for k in [\n        "total_lines","nonempty_lines","blank_lines","comment_lines",\n        "total_chars","total_bytes_utf8","avg_line_length","longest_line_length",\n        "trailing_whitespace_lines","leading_tab_lines","has_bom",\n        "word_count","unique_words","unique_byte_values","entropy_bits_per_byte",\n    ]:\n        print(f"  - {k}: {t[k]}")\n    print("  - newline_counts:", t["newline_counts"])\n    print("  - char_classes:", t["char_classes"])\n    print("  - top_bytes (value, count):", t["top_bytes"])\n\n    print("AST metrics:")\n    print(f"  - function_count: {a[\'function_count\']}")\n    print(f"  - class_count: {a[\'class_count\']}")\n    print(f"  - import_count: {a[\'import_count\']}")\n    print(f"  - rough_cyclomatic: {a[\'rough_cyclomatic\']}")\n    print(f"  - node_type_counts_top10: {a[\'node_type_counts_top10\']}")\n    print(f"  - top_identifiers: {a[\'top_identifiers\']}")\n    if a["function_spans"]:\n        print("  - function_spans:")\n        for name, start, end in a["function_spans"]:\n            print(f"      * {name}: lines {start}..{end}")\n    print(f"  - longest_function_span: {a[\'longest_function_span\']}")\n\n# ------------------------------ Quine ----------------------------------------\ndef print_full_file_quine():\n    divider("FULL-FILE QUINE (no file I/O)")\n    # FRAME contains the entire file with a single %%r placeholder.\n    # Passing FRAME into its own formatter fills that placeholder with repr(FRAME).\n    src = FRAME %% FRAME\n    print(src, end="")\n\n# ---------------------------------- Main -------------------------------------\ndef main():\n    # Build the exact source in memory using the quine mechanism:\n    SOURCE = FRAME %% FRAME\n    # 1) Self-metrics from in-memory source\n    print_metrics(SOURCE)\n    # 2) Print the full-file quine\n    print_full_file_quine()\n    divider("DONE")\n\nif __name__ == "__main__":\n    main()\n'

import ast
import hashlib
import math
import re
import sys
from collections import Counter
from textwrap import indent

# ------------------------------ Utilities ------------------------------------
def divider(title=None):
    bar = "=" * 72
    if title:
        print(f"\n{bar}\n{title}\n{bar}")
    else:
        print(f"\n{bar}")

def sha256_hex(data: str) -> str:
    return hashlib.sha256(data.encode("utf-8")).hexdigest()

# ------------------------------ Metrics --------------------------------------
def compute_text_metrics(src: str) -> dict:
    # Basic counts
    lines = src.splitlines()
    total_lines = len(lines)
    total_chars = len(src)
    total_bytes = len(src.encode("utf-8"))
    nonempty = sum(1 for ln in lines if ln.strip())
    blank = total_lines - nonempty
    comment = sum(1 for ln in lines if re.match(r"^\s*#", ln))

    # Whitespace styles
    trailing_ws = sum(1 for ln in lines if len(ln) and ln.rstrip() != ln)
    leading_tabs = sum(1 for ln in lines if ln.startswith("\t"))
    has_bom = 1 if src.startswith("\ufeff") else 0

    # Newline style (heuristic counts)
    crlf = src.count("\r\n")
    lf = src.count("\n") - crlf
    cr = src.count("\r") - crlf

    # Line lengths
    longest = max((len(ln) for ln in lines), default=0)
    avg_line = (total_chars / total_lines) if total_lines else 0.0

    # Token-ish word counts
    words = re.findall(r"[A-Za-z0-9_]+", src)
    word_count = len(words)
    uniq_words = len(set(w.lower() for w in words))

    # Character classes
    classes = {
        "letters": sum(c.isalpha() for c in src),
        "digits": sum(c.isdigit() for c in src),
        "whitespace": sum(c.isspace() for c in src),
        "punct": total_chars - sum(c.isalnum() or c.isspace() for c in src),
    }

    # Byte stats
    b = src.encode("utf-8")
    byte_counts = Counter(b)
    top_bytes = byte_counts.most_common(10)
    unique_byte_values = len(byte_counts)

    # Rough Shannon entropy (bits per byte)
    total_b = len(b) or 1
    entropy = 0.0
    for _, cnt in byte_counts.items():
        p = cnt / total_b
        entropy -= p * math.log2(p)

    return {
        "total_lines": total_lines,
        "nonempty_lines": nonempty,
        "blank_lines": blank,
        "comment_lines": comment,
        "total_chars": total_chars,
        "total_bytes_utf8": total_bytes,
        "avg_line_length": round(avg_line, 3),
        "longest_line_length": longest,
        "trailing_whitespace_lines": trailing_ws,
        "leading_tab_lines": leading_tabs,
        "has_bom": bool(has_bom),
        "newline_counts": {"CRLF": crlf, "LF": lf, "CR": cr},
        "word_count": word_count,
        "unique_words": uniq_words,
        "char_classes": classes,
        "unique_byte_values": unique_byte_values,
        "entropy_bits_per_byte": round(entropy, 4),
        "top_bytes": top_bytes,
    }

def compute_ast_metrics(src: str) -> dict:
    tree = ast.parse(src)
    counts = Counter(type(n).__name__ for n in ast.walk(tree))

    funcs = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
    classes = [n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
    imports = [n for n in ast.walk(tree) if isinstance(n, (ast.Import, ast.ImportFrom))]

    # Rough cyclomatic proxy: decisions + boolean ops + comprehensions + 1
    decisions = sum(isinstance(n, (ast.If, ast.For, ast.While, ast.Try, ast.With)) for n in ast.walk(tree))
    bool_ops = sum(isinstance(n, ast.BoolOp) for n in ast.walk(tree))
    comps = sum(isinstance(n, (ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp)) for n in ast.walk(tree))
    rough_cyclomatic = 1 + decisions + bool_ops + comps

    # Identifier frequencies
    names = [n.id for n in ast.walk(tree) if isinstance(n, ast.Name)]
    top_identifiers = Counter(names).most_common(10)

    # Function line spans (if available)
    def_spans = []
    for n in funcs:
        start = getattr(n, "lineno", None)
        end = getattr(n, "end_lineno", None)
        if start is not None:
            def_spans.append((n.name, start, end))

    # Longest function by line span
    longest_fn = None
    if def_spans:
        longest_fn = max(def_spans, key=lambda t: (t[2] or t[1]) - t[1] + 1 if t[2] else 0)

    return {
        "node_type_counts_top10": counts.most_common(10),
        "function_count": len(funcs),
        "class_count": len(classes),
        "import_count": len(imports),
        "rough_cyclomatic": rough_cyclomatic,
        "top_identifiers": top_identifiers,
        "function_spans": def_spans,
        "longest_function_span": longest_fn,
    }

def print_metrics(src: str):
    divider("SELF-METRICS")
    print(f"SHA-256: {sha256_hex(src)}")

    t = compute_text_metrics(src)
    a = compute_ast_metrics(src)

    print("Text metrics:")
    for k in [
        "total_lines","nonempty_lines","blank_lines","comment_lines",
        "total_chars","total_bytes_utf8","avg_line_length","longest_line_length",
        "trailing_whitespace_lines","leading_tab_lines","has_bom",
        "word_count","unique_words","unique_byte_values","entropy_bits_per_byte",
    ]:
        print(f"  - {k}: {t[k]}")
    print("  - newline_counts:", t["newline_counts"])
    print("  - char_classes:", t["char_classes"])
    print("  - top_bytes (value, count):", t["top_bytes"])

    print("AST metrics:")
    print(f"  - function_count: {a['function_count']}")
    print(f"  - class_count: {a['class_count']}")
    print(f"  - import_count: {a['import_count']}")
    print(f"  - rough_cyclomatic: {a['rough_cyclomatic']}")
    print(f"  - node_type_counts_top10: {a['node_type_counts_top10']}")
    print(f"  - top_identifiers: {a['top_identifiers']}")
    if a["function_spans"]:
        print("  - function_spans:")
        for name, start, end in a["function_spans"]:
            print(f"      * {name}: lines {start}..{end}")
    print(f"  - longest_function_span: {a['longest_function_span']}")

# ------------------------------ Quine ----------------------------------------
def print_full_file_quine():
    divider("FULL-FILE QUINE (no file I/O)")
    # FRAME contains the entire file with a single %r placeholder.
    # Passing FRAME into its own formatter fills that placeholder with repr(FRAME).
    src = FRAME % FRAME
    print(src, end="")

# ---------------------------------- Main -------------------------------------
def main():
    # Build the exact source in memory using the quine mechanism:
    SOURCE = FRAME % FRAME
    # 1) Self-metrics from in-memory source
    print_metrics(SOURCE)
    # 2) Print the full-file quine
    print_full_file_quine()
    divider("DONE")

if __name__ == "__main__":
    main()

========================================================================
DONE
========================================================================
