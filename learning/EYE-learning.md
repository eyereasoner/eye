# A Guide to EYE Learning

## The Challenge: From Raw Data to Trusted Insight

In an era of powerful AI, the path from raw **Data** to trusted, actionable insight remains a significant challenge. How can we be sure an answer is correct? How can we audit the reasoning behind it? And how can we automate this process reliably?

**EYE Learning** offers a practical and robust solution. It is a method for transforming raw inputs—**Data**, **Logic**, and a specific **Goal**—into a single, self-contained Python program. This program is generated by a Large Language Model (LLM) acting as a *meta-compiler* and is engineered to be fully autonomous.

Most importantly, it delivers three key outputs, creating a "triad of trust":

1.  The final **Answer**.
2.  A clear explanation of the **Reason Why**.
3.  An independent **Check (harness)** that validates the result, guarding against errors and LLM hallucinations.

A [public repository](https://github.com/eyereasoner/eye/tree/master/learning) provides a comprehensive suite of examples and a one-command runner to reproduce this "answer, reason, check" triad for every case.

-----

## What EYE Learning Is: Goal-Directed Program Synthesis

At its core, EYE learning is a pattern for **goal-directed program synthesis**. Think of it like giving a brilliant, lightning-fast programmer a very specific set of instructions.

The process is simple yet powerful:

1.  You declare a precise **Goal** (e.g., "Is this transaction compliant?" or "Find all ancestors of person X.").
2.  You provide the necessary **Data** and the **Logic** that governs the domain.
3.  You instruct the LLM to write **one Python file** that automatically ingests the inputs and produces the complete "answer, reason, check" output.

The key deliverable is a **self-contained and self-verifying program**. It's not just a code snippet to be integrated into a larger system; it's a trustworthy and auditable artifact you can execute in a CI/CD pipeline, share with auditors, and deploy with confidence.

-----

## What Makes It Different: A Hybrid Approach

EYE learning stands out by combining the creative flexibility of generative AI with the formal rigor of symbolic systems. This hybrid approach offers several unique advantages.

  * **Self-Contained, Self-Checking Outputs:** The LLM's primary output is a single, runnable Python program with its own built-in test harness. Every execution produces both a result and an independent verification. This builds immense trust and reliability, moving beyond the "black box" paradigm.

  * **A Bridge Between Symbolic and Generative AI:** It uses the LLM for what it does best—understanding intent and synthesizing code structure—while relying on formal **Logic** for explainability. This overcomes the brittleness of pure symbolic systems and the opacity of pure neural networks.

  * **Explainability by Design:** The generated program is explicitly required to explain its reasoning. This aligns with the core philosophy of the EYE reasoner [^1], which provides transparent logical derivations instead of just answers. You don't just know *what* the answer is; you know *why* it's the answer.

  * **Goal-First Engineering:** By starting with a clear business or research goal, the LLM learns a concrete, repeatable procedure to achieve it. The resulting program becomes a durable asset—perfect for automation, compliance checks, and reproducible research.

-----

## Architecture at a Glance

The conceptual pipeline is straightforward. The inputs are fed into an LLM, which acts as a synthesizer to produce the final, executable program. This program is the sole artifact needed to generate actionable insight.

![EYE learning](EYE-learning.png)

This architecture is built on two principles: (1) runtime **verification is mandatory**, and (2) the primary output is a **portable program** that is easy to manage, version, and execute anywhere.

-----

## Advanced Pattern: Mixed Computation

For performance-critical applications, EYE learning supports an advanced pattern that separates stable **Logic** from dynamic **Data**. This **"mixed-computation"** approach, inspired by foundational computer science principles [^2], treats your stable policies (the "how") as static code and your live inputs (the "what") as dynamic data.

The LLM-guided synthesis step acts as a "specializer," converting the declarative **Logic** into a compact, highly efficient **Driver** function in Python.

  * **At Runtime:** This specialized Driver is extremely fast. It consumes only the dynamic facts (e.g., a new user transaction), applies the pre-compiled logic, and emits the standard **Answer**, **Reason Why**, and **Check**.
  * **Governance:** The core logic remains in a human-readable, declarative format. To update a policy, you simply update the logic and re-run the synthesis step to generate a new Driver—no complex algorithmic rewrite is needed.
  * **Benefits:** This approach preserves the core "answer, reason, check" contract while dramatically improving **speed, determinism, and auditability**. Your logic stays declarative and clear, while your execution becomes small, fast, and predictable.

-----

## Getting Started: A Typical Workflow

Adopting EYE Learning is a straightforward, iterative process:

1.  **Define the Goal:** Start by clearly stating the decision, conclusion, or question you need to answer.
2.  **Assemble Inputs:** Gather the relevant data files and the logic that defines your operational constraints.
3.  **Synthesize the Program:** Use a prompt to instruct the LLM to generate the single Python program that produces the answer, reason, and check.
4.  **Execute and Validate:** Run the generated program. Confirm that the outputs are correct and that the self-verification harness passes. The repository's `./test` command automates this for all examples.
5.  **Iterate and Harden:** As your data and logic evolve, simply re-run the synthesis step to create an updated, validated artifact.

-----

## Why This Matters: The Practical Benefits

  * **Trust and Auditability:** The "answer, reason, check" triad provides a robust framework for building trust in AI-driven systems. Every result is verifiable and explainable, making it suitable for regulatory and compliance-driven environments.
  * **Extreme Automation:** By producing a self-contained executable, EYE Learning is a perfect fit for modern DevOps and MLOps pipelines. The generated programs can be versioned in Git, tested in CI, and deployed anywhere.
  * **Lower Maintenance Overhead:** Because the system's intelligence is maintained as declarative logic, you don't need to refactor complex code to change business policies. You simply update the logic and regenerate the program.
  * **Democratized Expertise:** This pattern allows subject matter experts to define operational logic in a high-level format, while the LLM handles the complex task of translating that logic into efficient, verifiable code.

[^1]: R. Verborgh and J. De Roo. Drawing Conclusions from Linked Data on the Web: The EYE Reasoner. IEEE Software, vol. 32, no. 3, pp. 23-27, May-June 2015, doi: 10.1109/MS.2015.63.
[^2]: Ershov, A. P. (1982). Mixed Computation: Potential Applications and Problems for Study. Theoretical Computer Science, 18, 41–67.

